# A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method

[![arXiv](https://img.shields.io/badge/arXiv-2602.02320-b31b1b.svg)](https://arxiv.org/abs/2602.02320)
[![Hugging Face](https://img.shields.io/badge/ðŸ¤—%20Datasets-ChemFM%2FMolLangData-yellow)](https://huggingface.co/datasets/ChemFM/MolLangData)
[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

**MolLangData** provides large-scale paired data of molecular structures and language descriptions generated by a rule-regularized method.

---

## Dataset

- **Hugging Face:** [ChemFM/MolLangData](https://huggingface.co/datasets/ChemFM/MolLangData) â€” The dataset is available here. We are still actively collecting more data beyond the currently 163k samples in MolLangData. 
- **Related benchmark:** [MolLangBench](https://github.com/TheLuoFengLab/MolLangBench) â€” human-curated benchmark for molecular structure recognition, editing, and generation. The generation task is similar to the structural description in this work but can be regarded as a standard evaluation, since it is human-curated and rigorously validated.

---

## Collaboration Wanted

We are actively continuing research on molecularâ€“language alignment, including dataset curation and multimodal model development.

We welcome collaborators who are interested in working together in this area.  Please contact [Feiyang Cai](mailto:feiyang@clemson.edu) to discuss potential collaboration.

---

## OPSIN (IUPAC â†’ XML / SMILES)

We use a customized [OPSIN](https://github.com/feiyang-cai/opsin_mollangdata) fork that adds **complete XML structure metadata** for building prompts:

- **Fork:** [feiyang-cai/opsin_mollangdata](https://github.com/feiyang-cai/opsin_mollangdata)
- The code there is for reference; we provide a **compiled JAR** for easy use in the simple demo (see below).
- **OPSIN README** and usage instructions will be updated later (see [TODO](#todo--roadmap)).

---

## Requirements

- **Python**: 3.8+
- **Java**: a working `java` runtime on PATH (JRE/JDK)
- **Python dependencies**: `pip install -r requirements.txt` (installs `openai`, `rdkit`). 

---

## Single molecule: get prompt and description from IUPAC

The script **`get_prompt_description_from_iupac.py`** takes **one IUPAC name**, runs OPSIN to get XML and SMILES, computes the **difficulty level** (easy / medium / hard) from the structure, builds the prompt, and optionally calls an LLM to get a structure description.

**Contents**

- `get_prompt_description_from_iupac.py` â€” main script
- `jar/` â€” place the OPSIN jar here: `jar/opsin-core-*-jar-with-dependencies.jar`
- `prompts/smiles_iupac_metadata_v13/` â€” prompt template and semantic sections (default, bridged/fused/spiro ring semantics)
- `config/llm_config.json` â€” LLM backend (Azure / OpenAI), model and reasoning effort per difficulty (easy/medium/hard), timeouts, etc. (used only with `--get-description`)

**Run** (from repo root)

Print OPSIN output, difficulty level, and prompt only (no LLM):

```bash
python3 get_prompt_description_from_iupac.py "3,4-dihydro-2H-1,5-benzodioxepin-7-yl-(2-fluorophenyl)methanone"
```

Get description from LLM (model/reasoning chosen by difficulty level; Azure or OpenAI per config) and print everything:

```bash
python3 get_prompt_description_from_iupac.py "3,4-dihydro-2H-1,5-benzodioxepin-7-yl-(2-fluorophenyl)methanone" --get-description
```

Write output to a folder (prompt â†’ `prompt.md`, description â†’ `descriptions.txt`, metadata â†’ `generation_info.json`):

```bash
python3 get_prompt_description_from_iupac.py "ethane" --get-description -o out/
```

<details>
<summary><strong>Options</strong></summary>

- `--opsin-jar PATH` â€” path to OPSIN jar (default: `jar/opsin-core-*-jar-with-dependencies.jar`)
- `--prompts-dir DIR` â€” prompt templates directory (default: `prompts/smiles_iupac_metadata_v13`)
- `-o DIR` / `--output DIR` â€” output folder: writes `prompt.md`, and if `--get-description` also `descriptions.txt` and `generation_info.json`; if omitted, everything is printed to stdout
- `--get-description` â€” call LLM to generate structure description (uses `config/llm_config.json` unless overridden)
- `--llm-config FILE` â€” LLM config JSON (default: `config/llm_config.json`)
- `--model MODEL` â€” override model for LLM
- `--reasoning-effort EFFORT` â€” override reasoning effort for LLM

</details>

<details>
<summary><strong>Output</strong></summary>

- Without `-o`: prints IUPAC, OPSIN status, SMILES, difficulty level, and (if `--get-description`) description and prompt.
- With `-o DIR`: writes `DIR/prompt.md`, and when using `--get-description` also `DIR/descriptions.txt` and `DIR/generation_info.json` (backend, model, reasoning effort, duration, etc.).

</details>

---

## Dataset generation

We provide pre-sampled PubChem data and auxiliary tools for the batch prompt generation pipeline.

**Pre-sampled data (8 rounds, 200k samples per round)** â€” download and use as input for downstream steps, or reproduce via Step 1 and Step 2:

- **Box:** [MolLangData PubChem sampled TSV](https://clemson.box.com/s/5ioww4x9273pscfqtzmnpze80j9b8ugh)  
  Each round folder contains `sampled.tsv` (e.g. `round_0/sampled.tsv`, `round_1/sampled.tsv`, â€¦).

**Step 1 â€” SDF â†’ TSV:** `batch_prompt_generation/miscellaneous/1_sdf_to_tsv` â€” convert PubChem SDF files to TSV with selected columns.

**Step 2 â€” Sampling:** `batch_prompt_generation/miscellaneous/2_sampling_from_pubchecm` â€” deterministic sampling from TSV folders into multiple chunks (e.g. 200k per round).

**Step 3 â€” OPSIN + MolLangData parsing:** Run the custom [OPSIN](https://github.com/feiyang-cai/opsin_mollangdata) tool on each sampled chunk to obtain full XML structure data for every molecule. Input: a folder that contains `sampled.tsv` (e.g. a round folder from Step 2 or from the pre-sampled Box data). Output: written under that folder (or a custom `--out-dir`) as `parsing_out_mollangdata_<version>/`, containing `opsin_xml/`, `mollangdata_xml/`, and `parsing_results.tsv`. Pre-run parsing output is also on Box (e.g. `round_1/parsing_out_mollangdata_0.1.3`).

The script resolves the OPSIN JAR from the repository: it looks in `opsin/opsin-core/target/` or `jar/` under the repo root. You can override with `--opsin-jar`.

**Run** (from repo root; replace `<sample_folder>` with a round folder that contains `sampled.tsv`):

```bash
python3 batch_prompt_generation/3_run_opsin_mollangdata_on_sampled.py <sample_folder>
```

<details>
<summary><strong>Step 3 â€” Arguments</strong></summary>

- `sample_folder` â€” Folder containing `sampled.tsv` (required). TSV must have columns: `PUBCHEM_COMPOUND_CID`, `SMILES`, `canonical_smiles`, and an IUPAC column (see `--iupac-column`).
- `--opsin-jar PATH` â€” Path to OPSIN MolLangData JAR (optional). If omitted, the script looks under the repo in `opsin/opsin-core/target/` or `jar/` for `opsin-core-*-mollangdata-*-SNAPSHOT-jar-with-dependencies.jar` or `opsin-core-*-jar-with-dependencies.jar`.
- `--out-dir DIR` â€” Output directory (optional). Default: `<sample_folder>/parsing_out`. The actual output is written under `parsing_out_mollangdata_<version>/` inside this dir.
- `--iupac-column NAME` â€” TSV column to use for IUPAC names. Default: `PUBCHEM_IUPAC_SYSTEMATIC_NAME`. Can use e.g. `PUBCHEM_IUPAC_NAME`.
- `--timeout-s N` â€” Timeout in seconds per molecule Java call. Default: 30.
- `--max-rows N` â€” Stop after processing N data rows (optional; for quick tests).
- `--verbose` â€” Verbose logging.

</details>

**Step 4 â€” Create batch prompt JSONL:** Build LLM job files from Step 3 output. The script assigns a **difficulty level** (easy / medium / hard) per molecule, builds the **prompt** with the same dynamic template as the single-molecule demo (including fused/spiro/bridged sections when applicable), and **routes** model and reasoning effort from `config/llm_config.json`. It writes JSONL (and optional per-prompt TXT) suitable for later one-by-one or batch LLM runs. Supports **Azure and OpenAI** backends and both **Chat Completions** and **Responses** APIs. Pre-built prompts for the 8 chunks are on Box (e.g. `round_1/parsing_out_mollangdata_0.1.3_prompts_jobs`).

**Run** (from repo root; `<input_folder>` = Step 3 output folder containing `parsing_results.tsv`, e.g. `parsing_out_mollangdata_0.1.3`):

```bash
python3 batch_prompt_generation/4_create_batch_prompt_jsonl.py <input_folder> <output_folder> [prompts_folder] --api-format responses
```

<details>
<summary><strong>Step 4 â€” Arguments</strong></summary>

- `input_folder` â€” Folder that contains `parsing_results.tsv` (and sibling `mollangdata_xml/` or `opsin_xml/`). Usually the Step 3 output directory (e.g. `parsing_out_mollangdata_0.1.3`).
- `output_folder` â€” Where to write JSONL files (and optional TXT prompts). Files are split by model and reasoning effort.
- `prompts_folder` â€” (Optional.) Folder with prompt templates (default: `prompts/smiles_iupac_metadata_v13` at repo root).
- `--llm-config FILE` â€” Path to LLM config JSON (default: `config/llm_config.json`). Must have top-level `easy`, `medium`, `hard` with `model` and `reasoning_effort`.
- `--api-format {responses|chat_completions}` â€” Which API format to emit in the JSONL. Default: `chat_completions`. Use `responses` for Responses API (`/v1/responses`).
- `--chat-completions-url URL` â€” URL for chat-completions entries (default: `/v1/chat/completions`).
- `--responses-url URL` â€” URL for responses entries (default: `/v1/responses`).
- `--xml-type {mollangdata_xml|opsin_xml}` â€” Which XML subfolder to use (default: `mollangdata_xml`).
- `--exclude-iupac` â€” Omit the **IUPAC Name:** block from prompts.
- `--exclude-xml` â€” Omit the **XML Metadata:** block from prompts.
- `--allow-dots` â€” Include rows whose SMILES contain a dot (disconnected component).
- `--sample-size N` â€” Randomly sample N rows (optional; for testing).
- `--random-seed N` â€” Seed for sampling (default: 533).
- `--custom-prefix STR` â€” Prefix for compound IDs in `custom_id`.
- `--no-txt` â€” Do not write per-prompt `.txt` files; write only JSONL.

</details>

<details>
<summary><strong>Step 4 â€” Backend / API recommendations (please read)</strong></summary>

- **Azure:** GPT-5.2 does not support batch mode on Azure at the time of writing. We run requests **one-by-one** using **background mode**, which is only supported by the **Responses API**. So for Azure we recommend **`--api-format responses`**. This is what we used for data generation.
- **OpenAI:** Both Responses and Chat Completions work. However, using the **Batch API with the Responses API** has been reported to cause tasks to run repeatedly and incur extra cost (see [Batch API task runs repeatedly with gpt-5.2-pro](https://community.openai.com/t/batch-api-task-runs-repeatedly-with-gpt-5-2-pro/1371203)). We therefore recommend using the **Completions API** (`--api-format chat_completions`) when running with OpenAI.

</details>

**Step 5 â€” Run LLM jobs:** Two options.

- **5a â€” OpenAI Batch API** (`5a_submit_openai_batch_jobs.py`): Upload JSONL to OpenAI Batch API, wait for completion, and retrieve results. **OpenAI only.** Batch pricing is **half** the on-demand price, so if you use **OpenAI**, prefer 5a. Requires `OPENAI_API_KEY`.
- **5b â€” One-by-one (Azure or OpenAI)** (`5b_run_requests_one_by_one.py`, `5b_run_tmux_jobs.sh`): Run each request sequentially (or in parallel via tmux). Supports **both Azure and OpenAI**. We use 5b because **batch mode is not supported for GPT-5.2 on Azure** at the time of writing. You can run **multiple processes** (e.g. split one JSONL into several tmux windows) to speed up collection. Both Responses and Completions are supported; we recommend **Responses** because **background mode** (submit â†’ poll) is more stable during data collection and is only supported by the Responses API.

**5a â€” Example** (from repo root; OpenAI, batch, wait and retrieve):

```bash
python3 batch_prompt_generation/5a_submit_openai_batch_jobs.py ./jobs_dir --output-dir ./batch_results
```

**5b â€” Example** (one process, Azure, resume with pool):

```bash
python3 batch_prompt_generation/5b_run_requests_one_by_one.py ./jobs_dir --backend azure --output-dir ./runs/run1 --resume --pool-folder ./runs/pool
```

**5b â€” Example** (multiple processes via tmux; split one JSONL into 4 workers):

```bash
./batch_prompt_generation/5b_run_tmux_jobs.sh -f ./jobs_responses_gpt-5.2-xhigh-easy.jsonl -s myrun -n 4 -o ./runs_out
```

<details>
<summary><strong>Step 5a â€” Arguments (OpenAI Batch)</strong></summary>

- `input` â€” Path to a single `.jsonl` file or a directory of `.jsonl` files (from Step 4).
- `--output-dir DIR` â€” Where to save retrieved output/error files (default: `batch_results`).
- `--no-wait` â€” Submit only; do not wait for completion or download results (default: wait and retrieve).
- `--poll-interval N` â€” Seconds between status polls when waiting (default: 60).
- `--max-requests N` â€” Max requests per batch file when splitting large JSONL (default: 50000; OpenAI limit).
- `--completion-window DUR` â€” Batch completion window (default: `24h`).
- `--dry-run` â€” List files and endpoints only; do not upload or create batches.

</details>

<details>
<summary><strong>Step 5b â€” Arguments (one-by-one runner)</strong></summary>

- `input` â€” A `.jsonl` file or a directory containing `.jsonl` files.
- `--output-dir DIR` â€” **(Required.)** Directory for `results_<backend>.jsonl`, `stats_<backend>.jsonl`, and `request_outputs/<custom_id>/`.
- `--llm-config FILE` â€” Path to LLM config (default: `config/llm_config.json`). Used for backend, timeout, poll interval, and per-difficulty model/reasoning when not overridden.
- `--backend {openai|azure}` â€” Which backend to call (default: from config).
- `--resume` â€” Skip `custom_id`s already in results JSONL or in `request_outputs/<custom_id>/result.json`.
- `--pool-folder DIR` â€” When `--resume`: also skip requests completed in any `request_outputs/` under this folder (e.g. another run or a pool of runs).
- `--max-requests N` â€” Stop after N requests (optional).
- `--timeout N`, `--poll-interval N`, `--print-interval N` â€” Override config (seconds).
- `--model MODEL`, `--reasoning-effort LEVEL` â€” Override model/reasoning for all requests (default: from JSONL body).
- `--no-tqdm` â€” Disable progress bar.
- `--max-retries N`, `--retry-sleep SEC`, `--sleep SEC` â€” Retry and throttling (defaults from config or 2.0 / 0.0).

</details>

<details>
<summary><strong>Step 5b â€” Arguments (tmux multi-process)</strong></summary>

**Required:** `-f FILE` (input JSONL), `-s SESSION_NAME` (tmux session), `-n N` (number of splits = number of parallel workers).

**Optional:** `-o OUTPUT_BASE` (base dir for `<session>_job0`, â€¦; default: same as input file dir), `-p POOL_FOLDER` (resume: skip completed in pool), `--llm-config`, `--backend`, `--model`, `--reasoning-effort`, `-i START_IDX` (Azure: deployment start index, default 1), `-t TIMEOUT`, `--no-tqdm`. Defaults for backend/model/reasoning come from `config/llm_config.json`; CLI overrides them. Each worker runs `5b_run_requests_one_by_one.py` on a split of the JSONL; for Azure, workers use different deployment names (e.g. `gpt-5.2`, `gpt-5.2-2`, â€¦).

</details>

<details>
<summary><strong>Step 5 â€” Recommendations</strong></summary>

- **OpenAI:** Prefer **5a (Batch)** â€” half the price. Use 5b only if you need one-by-one (e.g. testing or when batch is unavailable).
- **Azure:** Use **5b (one-by-one)**; batch is not supported for GPT-5.2 on Azure at the time of writing.
- **API format:** Prefer **Responses API** for one-by-one runs (5b): background mode (submit then poll) is more stable and is only supported by the Responses API. Generate job JSONL with `--api-format responses` in Step 4 when you plan to use 5b.

</details>

(Scripts for Step 1 and Step 2 are provided without detailed usage here.)

---

## TODO / Roadmap

- [x] **Full pipeline to collect the data**
  - [x] **Simple generation code** â€” minimal script to go from IUPAC (or SMILES) + metadata to one description (single LLM call).
  - [x] **Routing example** â€” example of difficulty-based routing (easy / medium / hard) and model/reasoning settings.
  - [x] **Whole pipeline** â€” batch workflow: prepare prompts (e.g. from a molecule list), call batch API to generate descriptions, optional validation and export .
- [ ] **OPSIN** â€” README and usage instructions are maintained in [feiyang-cai/opsin_mollangdata](https://github.com/feiyang-cai/opsin_mollangdata); this repo will link to it and document how the JAR is used in our pipeline.

---

## Citation

If you use MolLangData in your research, please cite:

```bibtex
@article{MolLangData,
  title={A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method},
  author={Cai, Feiyang and He, Guijuan and Hu, Yi and Wang, Jingjing and Luo, Joshua and Zhu, Tianyu and Pilla, Srikanth and Li, Gang and Liu, Ling and Luo, Feng},
  year={2026},
  journal={arXiv preprint arXiv:2602.02320},
}
```

---

## License

This project is licensed under the [MIT License](LICENSE).

---

## Contact

- **Maintainer:** Feiyang Cai ([feiyang@clemson.edu](mailto:feiyang@clemson.edu))
